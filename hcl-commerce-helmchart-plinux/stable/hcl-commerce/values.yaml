######################################################
# Licensed Materials - Property of HCL Technologies
#  HCL Commerce
#  (C) Copyright HCL Technologies Limited 1996, 2024
######################################################

#####################################################
# SoFy has not been supported yet for ppc64le
# SoFy related deployment values
# Please leave them untouched in non-SoFy deployment
#####################################################
## [SoFy] global values used by solution deployment on SoFy
global:
  ## Image registry used by deployment on SoFy
  hclImageRegistry: hclcr.io/sofy
  hclImagePullSecret: ''

  ## Other global values used by deployment
  sofySolution: false
  ambassadorID: ""
  domain: ""
  persistence:
    rwxStorageClass: ""
    testRWXStorageClass: ""
  ## Ambassador is incompatible starting with Kubernetes 1.22 and must be migrated to a compatible version of Emissary
  emissaryID: ""

## [SoFy] Disable creating public routes for certain services in a Sofy solution
disablePublicRoute:
  store: false
  nifi: false
  ingest: false
  registry: false
  query: false

## [SoFy] Disable Access Control Service (authentication and authorization) for certain services in a Sofy solution
disableAccessControl:
  store: true
  nifi: true
  ingest: true
  registry: true
  query: true

#####################################################
# 3rd party Dependency components
#####################################################
## Override some values for zookeeper
zookeeper:
  enabled: false
  image:
    registry: icr.io
    repository: ppc64le-oss/zookeeper-ppc64le
    tag: v3.8.0-bv
  fullnameOverride: "hcl-commerce-zookeeper"
  replicaCount: 1
  maxClientCnxns: 120
  ## @param nodeSelector Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  resources:
    requests:
      cpu: 250m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 1024Mi

## The following values are used by the Redis chart.
## The provided configurations are for demonstration or non-production environments.
## The configurations (including memory) must be adjusted for production use.
## Review the following documents for details:
## https://github.com/HCL-TECH-SOFTWARE/hcl-commerce-performance/blob/main/HCL-Cache/README.md
## https://github.com/HCL-TECH-SOFTWARE/hcl-commerce-performance/blob/main/HCL-Cache/BitnamiRedisInstall.md
## If you set enabled to true, redis will be deployed to the same namespace as Commerce, 
## please make sure you update address of redis_cfg under hclCache to point to the correct redis:
##   address: "redis://hcl-commerce-redis-master.<redis name space>.svc.cluster.local.:6379"
##   for example:  address: "redis://hcl-commerce-redis-master.commerce.svc.cluster.local.:6379"
redis:
  enabled: false
  architecture: standalone
  fullnameOverride: "hcl-commerce-redis"
  replica:
    replicaCount: 0
  image:
    registry: icr.io/ppc64le-oss
    repository: redis-ppc64le
    tag: v7.4.2-bv
    pullPolicy: IfNotPresent
  ## Allow insecure image needs to set to true to use IBM built image
  global:
    security:
      allowInsecureImages: true
  auth:
    enabled: false
  networkPolicy:
    enabled: false
  serviceAccount:
    create: false
    name: "default"
  master:
    disableCommands: []
    serviceAccount:
      create: false
      name: "default"
    persistence:
      enabled: false
    ## @param master.nodeSelector Node labels for Redis&reg; master pods assignment
    ## ref: https://kubernetes.io/docs/user-guide/node-selection/
    ##
    nodeSelector: {}
    resources:
      limits:
        cpu: 2000m
        memory: 4Gi
      requests:
        cpu: 500m
        memory: 4Gi
    configuration: |-
      appendonly no
      save ""
      maxmemory 3000mb
      maxmemory-policy volatile-lru

## Override some values for elasticsearch
elasticsearch:
  enabled: false
  fullnameOverride: "hcl-commerce-elasticsearch"
  image: "icr.io/ppc64le-oss/elasticsearch-ppc64le"
  imageTag: "7.17.20"
  replicas: 1
  minimumMasterNodes: 1
  esJavaOpts: "-Xmx6g -Xms6g"
  nodeSelector: {}
  resources:
    requests:
      cpu: "1000m"
      memory: "6Gi"
    limits:
      cpu: "2000m"
      memory: "8Gi"
  esConfig:
    elasticsearch.yml: |
      indices.query.bool.max_clause_count: 100000
      xpack.monitoring.collection.enabled: true
      xpack.ml.enabled: false
      http.max_content_length: 1000mb

## Override some values for postgresql required when approval service is enabled
postgresql:
  enabled: false
  image: 
    registry: icr.io/ppc64le-oss
    repository: postgresql
    tag: v14.13.0-bv
  auth:
    postgresPassword: "Hvr9TJ86Rb4DwJTaBJnuPeD2DSJypQxs"
  fullnameOverride: "hcl-commerce-postgresql"
  ## init.sql is an sample script to initialize the postgresql database used by approval service
  ## Please do update the sample password in the script, and also make sure the datasource password under approval server section to be the same
  primary:
    initdb:
      scripts:
        init.sql: |-
          CREATE DATABASE comdb;
          CREATE USER hclcom WITH ENCRYPTED PASSWORD 'passw0rd';
          CREATE SCHEMA hclcom;
          GRANT ALL PRIVILEGES ON DATABASE comdb TO hclcom;
          GRANT USAGE ON SCHEMA hclcom TO hclcom;
          GRANT CREATE ON SCHEMA hclcom TO hclcom;
    resources:
      limits:
        memory: 2Gi
        cpu: 1000m
      requests:
        memory: 2Gi
        cpu: 1000m
    containerSecurityContext:
      runAsUser: 1001

## Override some values for vaultconsul, keep it false if not deployed with sofy
vaultconsul:
  enabled: false

## Currently not support yet for ppc64le
## Override value for nfs server provisioner
commercenfs:
  enabled: false
  persistence:
    enabled: true
    storageClass: ""
    size: 5Gi
  storageClass:
    create: false
    name: commercenfs-storage-class
    allowVolumeExpansion: true
    mountOptions:
      - vers=4.1

#####################################################
# Commerce deployment values for ppc64le
#####################################################

## Default values for HCL Commerce V9
## This is a YAML-formatted file.
## Declare variables to be passed into your templates.
arch:
  ppc64le: "3 - Most preferred"
  amd64: "0 - Do not use"
  s390x: "0 - Do not use"

## To view the license, run "helm install license ./ --set license=view --dry-run"
## In order to deploy HCL Commerce, set the license to "accept" by accepting the license
license: not_accepted

## Common configuration for all component Docker
common:
  ## HCL Commerce Production Information
  productVersion: 9.1.18.0

  ## If specified it will add an environment variable to all the containers to specify the timezone to use
  ## in the format of: America/Toronto by following the ICANN TZ Database.
  ## If not specified, 'GMT' is used in all the containers by default.
  ##
  ## CAUTION: changing the timezone will impact on some business logics, such as marketing web activity and promotion
  ## start and end date/time, as they are set to the commerce server time and won't auto update based on this timezone
  ## setting. It is recommended to keep it empty if your commerce is already on production and never set it explicitly
  ## previously.
  timezone:

  ## Commerce environment info
  tenant: demo
  environmentName: qa

  ## Commerce environment instances to deploy [share|auth|live].
  ## It could be a single instance or a comma separated list of instances
  environmentType: auth

  ## Search type [solr|elastic]
  searchEngine: elastic

  ## The name of the secret which contains the vault token
  vaultTokenSecret: "vault-token-secret"

  ## DBType, options: db2, oracle
  dbType: db2

  ## The docker registry repository with "<docker_registry_domain>[:port]/" format
  imageRepo: my-docker-registry.io:5000/

  ## Default value of spiUserName is configured for Commerce Version 9 sample Db2 Docker.
  ## Please correct this value to match your spi user name configured in your environment
  ## Note this value could also be set on Vault with the path Tenant/EnvName/EnvType/spiUserName, and vault configuration will take precedence
  spiUserName: spiuser

  ## The AES encrypted value of the spiuser password
  ## This value can be obtained by running wcs_encrypt.sh utility from utility container
  ## Visit https://help.hcltechsw.com/commerce/9.0.0/install/tasks/tiginstall_definespi.html to find more details
  ## Setting this variable using Vault is recommended (vault configuration will also take precedence) instead of typing the sensitive value here, path on vault is Tenant/EnvName/EnvType/spiUserPwd
  spiUserPwdAes:

  ## Base64 encoded value for <spiuser>:<password>
  ## This value can be obtained by running "echo -n <spiuser>:<password> | base64"
  spiUserPwdBase64:

  ## Vault v1 api url. Following default value is for development mode vault deployed in vault name space
  vaultUrl: http://vault-consul.vault.svc.cluster.local:8200/v1

  ## External domain used for ingress and store preview URL
  ## For example. in hostname store.demoqaauth.mycompany.com , .mycompany.com would be the External Domain name.
  externalDomain: .mycompany.com

  ## Default value for BindingConfigMap; you can change the default config map name.
  ## If you use vault as configuration mode, you should set it as empty value
  bindingConfigMap:

  ## As default, configMode is Vault
  configureMode: Vault

  ## CAUTION: MAJOR UPDATE IN COMMERCE HELM CHART VERSION 9.1.14.0
  ## RECOMMENDED: Enable runAsNonRoot would start all the pods as the non-root user (comuser) to improve overall security
  ## However, if you are upgrading your existing env, be sure to check your existing customizations to make them compatible with the non-root user
  ## If you still want to use the root user to start all the pods, then set to false would start all the pods as the root user
  runAsNonRoot: 
    enabled: true
    ## When upgrading your existing env from using root user to non-root user for the first time
    ## Please enable the option below to update the ownership of the files and directories stored for assetsPVC
    ## This option will trigger a pre-upgrade job to update the ownership when runAsNonRoot and assetsPVC are enabled
    ## If your existing env prior to the upgrade does not enable assetsPVC, then set this to false
    migrateAssetsPvcFromRootToNonroot: true

  ## Input the imagePull Secret Name which created by admin, in case you have image pull access control cross namespace
  ## kubectl create secret docker-registry myregistrykey --docker-server=<cluster_CA_domain>:8500 --docker-username=<user_name> --docker-password=<user_password> --docker-email=<user_email>
  imagePullSecrets: ''

  ## If you need to force-pull Docker image, use Always
  imagePullPolicy: IfNotPresent

  ## Specify service account
  #serviceAccountName: default

  ## dataIngressEnabled has been deprecated and removed in hcl-commerce helm chart version 2.1.4
  ## When dataIngressEnabled is set to true, it will create ingress for data platform services, such as nifi, ingest and query services.
  ## Please keep it as false for production for security reason
  ## dataIngressEnabled: false

  ## ingressController has been moved to ingress.ingressController
  ## ingressController: nginx

  ## When migrating from V7 or V8, there is an option to deploy the old Aurora based store in transaction server
  ## In this case, set localStoreEnabled to be true to allow service and ingress to be configured properly
  ## By default it is not enabled as the default Aurora store comes with V9 is remote, i.e. run on it's own server
  localStoreEnabled: false

  ## When disabled, commerce apps will add java.net.preferIPv4Stack=true jvm arg
  ipv6Enabled: false

## Default to use vault as CA to issue certificate
vaultCA:
  enabled: true
  # dnsNameInSubjectAlternativeName: None
  # containerHostname: None

## ingressSecret has been moved to ingress.ingressSecret
## ingressSecret:
##   autoCreate: true
##   replaceExist: true

## In previous helmchart for Commerce 9.0.x.x, the deployment matches the following labels to select pods
##   app (WCSV9)
##   chart ({{ .Chart.Name }}, e.g ibm-websphere-commerce)
##   release ({{ .Release.Name }}, e.g demo-qa-auth)
##   heritage ({{ .Release.Service }}, e.g Helm)
##   component ({{ .Values.common.tenant }}{{ .Values.common.environmentName}}{{ .Values.common.environmentType }}{{.Values.xxApp.name}},
##              e.g demoqaauthcrs-app)
##   group ({{ .Values.common.tenant }}{{ .Values.common.environmentName}}{{ .Values.common.environmentType }}, e.g demoqaauth)
##
## In this helmchart, the app and chart values are updated. This would cause helm upgrade fail as the LabelSelector is immutable.
##
## To upgrade v9.0.x.x deployment to v9.1.x.x using this chart without downtime,
## specify the labels below to match the existing deployment.
##
## For new deployment, leave the values commented
##
backwardCompatibility:
  selector: {}
    ## In V9.0.x.x helmchart, app is specified as WCSV9 by default
    #app: WCSV9

    ## In V9.0.x.x helmchart, chart is specified as ibm-websphere-commerce by default
    #chart: ibm-websphere-commerce

    ## Specify any extra labels to match deployment to pods
    #extraSelector:
      #label1: value1
      #label2: value2

  ## Some of the Nginx and GKE ingress names got updated since V9.1.7.0
  ## When upgrading from a version prior to V9.1.7.0 to a newer version, enable this flag to trigger an upgrade job to clean up the old ingress definitions
  ## This will avoid conflict errors with ingress definitions during upgrade
  ingressFormatUpgrade:
    enabled: false

## HCL Cache is used by HCL Commerce V9 by default starting from V9.1.0.0
## The following detailed configuration will be used to create a config map and then be passed to each Commerce application
## If you deploy redis to a different namespace than redis, plesae make sure you update 
## address to "redis://hcl-commerce-redis-master.<redis name space>.svc.cluster.local.:6379" 
## The same configuration must be used to deploy auth, live and share. If you update this configmap, please
## make sure you re-deploy auth, live and share to have all commerce apps running with the same cache and redis configuration.
## For details see: https://github.com/HCL-TECH-SOFTWARE/hcl-commerce-performance/blob/main/HCL-Cache/Configuration.md
hclCache:
  configMap:
    # content for cache_cfg-ext.yaml
    cache_cfg_ext: |-
      redis:
        enabled: true
        yamlConfig: "/SETUP/hcl-cache/redis_cfg.yaml" # Please leave this line untouched
      # Cache configurations
      cacheConfigs:
        # Default configuration enables local and remote caching
        # See defaultCacheConfig in /SETUP/hcl-cache/cache_cfg.yaml for the out-of-the-box 
        # default configuration.
        # The following custom configurations extend and override the definitions in the
        # /SETUP/hcl-cache/cache_cfg.yaml file.
        services/cache/SampleCustomLocalOnlyCache:
          remoteCache:
            enabled: false
        services/cache/SampleCustomRemoteOnlyCache:
          localCache:
            enabled: false
    # content for redis_cfg.yaml
    redis_cfg: |-
      singleServerConfig:
        idleConnectionTimeout: 10000
        connectTimeout: 3000
        timeout: 1000
        retryAttempts: 3
        retryInterval: 500
        subscriptionsPerConnection: 5
        sslEnableEndpointIdentification: true
        sslProvider: "JDK"
        pingConnectionInterval: 30000
        keepAlive: true
        tcpNoDelay: true
        address: "redis://hcl-commerce-redis-master.redis.svc.cluster.local.:6379"
        subscriptionConnectionMinimumIdleSize: 1
        subscriptionConnectionPoolSize: 50
        connectionMinimumIdleSize: 24
        connectionPoolSize: 64
        database: 0
        dnsMonitoringInterval: 5000
        password: "${JNDI/ENCRYPTED:REDIS_PASSWORD_ENCRYPT:-}"
      threads: 16
      nettyThreads: 32
      referenceEnabled: true
      transportMode: "NIO"
      lockWatchdogTimeout: 30000
      keepPubSubOrder: true
      useScriptCache: false
      minCleanUpDelay: 5
      maxCleanUpDelay: 1800
      addressResolverGroupFactory: !<org.redisson.connection.DnsAddressResolverGroupFactory> {}

## CreateSampleConfig supports quick deploy with the HCL Commerce sample Db2 Docker image, which can be used with your authoring environments, If you deploy on different tenant and env, update the DBHOSTNAME.
createSampleConfig:
  enabled: false
  dbHostName: demoqaauthdb
  dbName: mall
  dbUser: wcs
  dbPass: wcs1
  dbPort: 50000
  dbaUser: db2inst1
  dbaPassEncrypt:
  dbPassEncrypt:

## Flag to enable metrics. Enabled by default
metrics:
  enabled: true
  ## Flag to add prometheus scraping annotations to pods. Disabled by default
  prometheusAnnotations:
    enabled: false
  ## Flag to enable service monitor. Disabled by default
  serviceMonitor:
    enabled: false
    ## Specify a namespace in which to install the ServiceMonitor resource.
    ## Default to use the same release namespace where commerce is deployed to
    # namespace: monitoring

    # interval between service monitoring requests
    interval: 15s

    ## Defaults to what's used if you follow CoreOS [Prometheus Install Instructions](https://github.com/helm/charts/tree/master/stable/prometheus-operator#tldr)
    ## [Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#prometheus-operator-1)
    ## [Kube Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#exporters)
    selector:
      prometheus: kube-prometheus

## DX has not been supported yet for ppc64le
## Flag to enable DX. Only effective when using elastic search
## In addition, nginx ingress has to be used to deploy DX. HCL Commerce currently does not support deploying DX with GKE ingress.
dx:
  enabled: false
  # DX routing service name, it could be haproxy or ambassador based on your DX versions, get the service name with kubectl get serivce.
  serviceName:
    # Auth DX routing service name
    auth: ""
     # Live DX routing service name
    live: ""
  namespace:
    # Auth DX should be deployed under the same cluster, specify its namespace here
    auth: ""
     # Live DX should be deployed under the same cluster, specify its namespace here
    live: ""

## Job to build index for elastic search
## If enabled, a job will be created with helm release to wait for all commerce services ready before triggering the index build.
## It will also monitor the index to complete.
## When pushToLiveEnabled is enabled, it will also push index to live (assuming live is also being deployed or already deployed).
## The job will timeout and fail after specified maxDuration if not index is not completed within timeout.
## Please make maxDuration value reasonable according to your data size and number of store data to index.
searchIndexCreation:
  enabled: false
  pushToLiveEnabled: false
  overalMaxDuration: 6000
  indexMaxDuration: 2400
  interval: 15
  maxRetry: 5
  txnMaxDuration: 1200
  nifiMaxDuration: 900
  ingestMaxDuration: 3600
  storeIds: "11,12,41"
  calculatePriceEnabled: false
  calculatePriceStoreIds: "12"

## Global Logging configuration applies to all deployed app
logging:
  jsonLogging:
    enabled: false

## Assets persistent volumes configuration
assetsPVC:
  enabled: false
  ## keep it empty would use the cluster default storageClass
  storageClass: ""
  storage: "5Gi"
  ## accessMode here must be ReadWriteMany
  accessMode: "ReadWriteMany"
  ## if existing claims are empty, assets PVC will be auto generated while deploying shared instance
  existingClaim:
    auth: ""
    live: ""

## Enable this feature for deployment using openshift.
## Ingress rules would be implemented using openshift routes.
openshiftDeployment:
  enabled: false
  ## The name of an SecurityContextConstraints resource the service account should have access to
  sccName: "privileged"
  ## destinationCACertificate used to validate ssl certificate for commerce routes
  ## Format
  ## destinationCACertificate: |-
  ##   -----BEGIN CERTIFICATE-----
  ##   [......]
  ##   -----END CERTIFICATE-----
  destinationCACertificate: |-

## Not supported for plinux
## Enable this feature for deployment when Google Anthos is enabled.
## This will disable istio sidecar injection for pre-install, pre-delete, create-index, nifi and ingest to avoid failures
anthosDeployment:
  enabled: false

## Ingress configuration
ingress:
  ## ingress enablement. Make it disabled for SoFy deployment
  enabled: true

  ## this needs to be set to true when you are planning to allow sapphire store to launch B2B approval tooling
  enableToolingForReactStore: true

  ## flag to enable the manage approval page for marketplace approval service, set to false by default for security
  enableManageApprovalPage: false

  ## use 'networking.k8s.io/v1beta1' for kubernetes between 1.16 and 1.19 (exclusive)
  ## use 'networking.k8s.io/v1' for kubernetes 1.19 and above
  ## see https://kubernetes.io/docs/reference/using-api/deprecation-guide/#ingress-v122 for details
  apiVersion: networking.k8s.io/v1beta1

  ## ingress controller [nginx|gke|ambassador|emissary]
  ##   nginx - nginx ingress controller
  ##   gke - cloud load balancing using HTTP(S) Load Balancer in GKE
  ##   ambassador - ambassador API gateway
  ##   emissary - emissary API gateway (Ambassador 1.X template files is incompatible starting with Kubernetes 1.22 and must migrate to Emissary 2.X files instead in the Commerce Helm Chart)
  ingressController: nginx

  ## ambassador ids list for the emissary listener listening on http and https ports
  ## keep it to be empty if you are only using the default ambassador id
  ## when you have different ambassador ids defined in the ingress configuration below for different components, then need to add them here to create listeners for different emissaries
  emissaryIdsList: []

  ## IngressSecret is used to specify whether Helm needs to auto-generate the secret for ingress. For production environment, you can choose generate the secret with real CA certificate.
  ingressSecret:
    autoCreate: true
    replaceExist: true

  ## ingress configuration
  ## domain:
  ##   required.
  ##   can use template language to reference other values.
  ## ingressClass:
  ##   optional.
  ##   keep ingressClass empty to use the default ingress class, i.e. 'nginx' for nginx ingress controller, 'gce' for gke ingress controller, and empty for ambassador or emissary id
  ##   otherwise, specify the custom ingress controller for each ingress.
  ##   For ambassador and emissary, specify the ambassadorId as the ingressClass. Keep it to be empty if you are using the default ambassador id.
  ## tlsSecret:
  ##   optional.
  ##   keep it empty to use the tls secret auto generated to match the ootb domain pattern.
  ##   Otherwise specify the tls secret for each ingress to match the domain name.
  ## customAnnotations:
  ##  optional.
  ##  only applicable to gke and nginx ingress to add custom annotations to the ingress definitions
  cmc:
    auth:
      domain: cmc.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}auth{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    live:
      domain: cmc.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}live{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    customAnnotations: {}
  tooling:
    domain: tooling.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}{{ $.Values.common.externalDomain }}
    ingressClass:
    tlsSecret:
    customAnnotations: {}
  accelerator:
    auth:
      domain: accelerator.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}auth{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    live:
      domain: accelerator.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}live{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    customAnnotations: {}
  admin:
    auth:
      domain: admin.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}auth{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    live:
      domain: admin.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}live{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    customAnnotations: {}
  org:
    auth:
      domain: org.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}auth{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    live:
      domain: org.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}live{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    customAnnotations: {}
  crs:
    auth:
      domain: store.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}auth{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    live:
      domain: store.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}live{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    customAnnotations: {}
  localstore:
    auth:
      domain: store.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}auth{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    live:
      domain: store.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}live{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    customAnnotations: {}
  reactstore:
    auth:
      domain: www.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}auth{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    live:
      domain: www.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}live{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    customAnnotations: {}
  reactstorepreview:
    auth:
      domain: store-preview.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}auth{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    live:
      domain: store-preview.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}live{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    customAnnotations: {}
  query:
    auth:
      domain: search.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}auth{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    live:
      domain: search.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}live{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    data:
      domain: query.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    customAnnotations: {}
  solrsearch:
    auth:
      domain: search.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}auth{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    live:
      domain: searchrepeater.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}live{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    customAnnotations: {}
  orchestration:
    auth:
      domain: orchestration.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}auth{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    live:
      domain: orchestration.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}live{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
  transaction:
    auth:
      domain: tsapp.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}auth{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    live:
      domain: tsapp.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}live{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    customAnnotations: {}
  graphql:
    auth:
      domain: graphql.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}auth{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    live:
      domain: graphql.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}live{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    customAnnotations: {}
  cache:
    auth:
      enabled: false
      domain: cache.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}auth{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    live:
      enabled: false
      domain: cache.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}live{{ $.Values.common.externalDomain }}
      ingressClass:
      tlsSecret:
    customAnnotations: {}
  nifi:
    enabled: false
    domain: nifi.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}{{ $.Values.common.externalDomain }}
    ingressClass:
    tlsSecret:
    customAnnotations: {}
  ingest:
    enabled: false
    domain: ingest.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}{{ $.Values.common.externalDomain }}
    ingressClass:
    tlsSecret:
    customAnnotations: {}
  registry:
    enabled: false
    domain: registry.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}{{ $.Values.common.externalDomain }}
    ingressClass:
    tlsSecret:
    customAnnotations: {}
  mustgather:
    enabled: false
    domain: mustgather.{{ $.Values.common.tenant }}{{ $.Values.common.environmentName }}{{ $.Values.common.externalDomain }}
    ingressClass:
    tlsSecret:
    customAnnotations: {}


#######################################################
## Following are individual application configuration #
#######################################################

tsDb:
  ## By default, the sample Db2 Docker image is used in the deployment.
  ## This sample DB2 should only be used for non-production environment.
  ## Please see https://help.hcltechsw.com/commerce/9.1.0/install/tasks/tiginstalldb2overview.html for details
  enabled: true
  name: ts-db
  image: commerce/ts-db
  tag: 9.1.18.0
  resources:
    requests:
      memory: 6144Mi
      cpu: 2
    limits:
      memory: 6144Mi
      cpu: 2
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}
  envParameters:
    auth: {}
    live: {}
  persistence:
    enabled: true
    existingClaim: ""
    # keep storageClass empty to use the cluster default storageClass
    storageClass: ""
    storage: "10Gi"
    accessMode: "ReadWriteOnce"
  userScripts:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}
  ## demoPack is only used in SoFy sandbox, keep it disabled in your deployment
  demoPack:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}

tsApp:
  name: ts-app
  replica: 1
  image: commerce/ts-app
  tag: 9.1.18.0
  resources:
    requests:
      memory: 5120Mi
      cpu: 500m
    limits:
      memory: 5120Mi
      cpu: 2
  ## uncomment following property and set a proper merchant key to overwrite the merchant key in transaction server
  #merchantKey:
  ## when using custom envParameters, use key: value format
  envParameters:
    auth: {}
    live: {}
  startupProbe:
    initialDelaySeconds: 200
  livenessProbe:
    initialDelaySeconds: 800
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}
  coresSharingPersistentVolumeClaim:
    auth: ""
    live: ""
  ## Flag to enable WebSphere Application Server (WAS) Admin Console. Disabled by default
  wasAdminConsole:
    enabled: false
  ## Flag to use quick way to start transaction server. Disabled by default.
  ## Set to true to run all WAS admin runengine commands together to enhance ts-app startup time
  ## Set to false to run WAS admin runengine command one by one.
  quickStart:
    enabled: false

  ## LDAP configuration. LDAP is disabled by default
  ## Set enabled flag to true to enable LDAP.
  ## 3 options to store LDAP properties:
  ##   1) on Vault: 
  ##      keep useVmmPropertiesFile and useConfigMapForVmmPropertiesFile as false
  ##      set LDAP properties on Vault, check https://help.hcltechsw.com/commerce/9.1.0/install/refs/rigvaultmetadata.html for details
  ##   2) use vmm.properties file: 
  ##      set useVmmPropertiesFile to true and useConfigMapForVmmPropertiesFile to false;
  ##      update /SETUP/ldap/properties/vmm.properties and build customized ts-app docker image 
  ##   3) use ConfigMap: 
  ##      set useConfigMapForVmmPropertiesFile to true
  ##      update ldap-vmm-<auth|live>.properties file in helmchart
  ldap:
    auth:
      enabled: false
      useVmmPropertiesFile: false
      useConfigMapForVmmPropertiesFile: false
    live:
      enabled: false
      useVmmPropertiesFile: false
      useConfigMapForVmmPropertiesFile: false

  ## Behavioral marketing event listener enablement. Disabled by default
  ## see https://help.hcltechsw.com/commerce/9.1.0/admin/tasks/tsbenable.html for details
  marketingEventListeners:
    enabled: false
  userScripts:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}
  ## demoPack is only used in SoFy sandbox, keep it disabled in your deployment
  demoPack:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}

searchAppMaster:
  name: search-app-master
  image: commerce/search-app
  ## CAUTION: MAJOR UPDATE IN COMMERCE 9.1.18.0 SOLR-BASED SEARCH
  ## In version 9.1.18.0, Solr-based search updates the Solr version from 7.3.0 to 9.7.0
  ## This introduces significant changes, please check below "Upgrading Solr version 7.3.0 to 9.7.0" link for details
  ## https://help.hcl-software.com/commerce/9.1.0/search/concepts/c970_solrupgrade.html
  ## 
  ## Deployment Options for Solr-based Search in 9.1.18.0
  ## Option 1: Use 9.1.18.0 Solr-based search
  ##   - Please review "Upgrading Solr version 7.3.0 to 9.7.0" page carefully
  ##   - Ensure you set ${VAULT_URL}/${TENANT}/${ENVIRONMENT}/softwareStack to open in Vault
  ##     The corresponding environment variable is SOFTWARE_STACK
  ##   - Update tag to 9.1.18.0
  ## 
  ## Option 2: Use 9.1.17.0 Solr-based search + 9.1.18.0 Commerce others
  ##   - This approach lets you upgrade to HCL Commerce 9.1.18.0 without adopting the Solr 9.7.0 update
  ##   - If you are using Oracle
  ##     Open a case with HCL Customer Support to request patch HC-60507
  ##     Build customized 9.1.17.0 search-app docker image with patch HC-60507
  ##   - Update tag to your 9.1.17.0 search-app
  tag: 
  resources:
    requests:
      memory: 4096Mi
      cpu: 500m
    limits:
      memory: 4096Mi
      cpu: 2
  ## when using custom envParameters, use key: value format
  envParameters: {}
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## persistentVolumeClaim is deprecated and will be removed in future version
  ## Please specify the existing pvc in searchAppMaster.persistence.existingClaim instead
  ## searchAppMaster.persistence.existingClaim will take precedence if specified
  persistentVolumeClaim: ""
  persistence:
    enabled: true
    existingClaim: ""
    # keep storageClass empty to use the cluster default storageClass
    storageClass: ""
    storage: "5Gi"
    # NOTE if any of the shards are enabled, then ReadWriteMany accessMode is required
    accessMode: "ReadWriteOnce"
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}
  coresSharingPersistentVolumeClaim: ""
  userScripts:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}
  ## demoPack is only used in SoFy sandbox, keep it disabled in your deployment
  demoPack:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}
  ## Enable parallel preprocessing and distributed indexing by enabling shards below
  ## Multiple search servers will be deployed with the same configurations as searchAppMaster to speed up the indexing (except persistence, nodeLabel, nodeSelector and coresSharingPersistentVolumeClaim)
  ## Ensure persistence is enabled and using ReadWriteMany accessMode is required
  shardA:
    enabled: false
    name: search-app-shard-a
    persistence:
      enabled: true
      existingClaim: ""
      # keep storageClass empty to use the cluster default storageClass
      storageClass: ""
      storage: "5Gi"
      # NOTE ReadWriteMany accessMode is required
      accessMode: "ReadWriteMany"
    ## the nodes must have a label with the key wc-node-select-flag
    ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
    nodeLabel: ""
    ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
    ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
    nodeSelector: {}
    coresSharingPersistentVolumeClaim: ""
  shardB:
    enabled: false
    name: search-app-shard-b
    persistence:
      enabled: true
      existingClaim: ""
      # keep storageClass empty to use the cluster default storageClass
      storageClass: ""
      storage: "5Gi"
      # NOTE ReadWriteMany accessMode is required
      accessMode: "ReadWriteMany"
    ## the nodes must have a label with the key wc-node-select-flag
    ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
    nodeLabel: ""
    ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
    ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
    nodeSelector: {}
    coresSharingPersistentVolumeClaim: ""
  shardC:
    enabled: false
    name: search-app-shard-c
    persistence:
      enabled: true
      existingClaim: ""
      # keep storageClass empty to use the cluster default storageClass
      storageClass: ""
      storage: "5Gi"
      # NOTE ReadWriteMany accessMode is required
      accessMode: "ReadWriteMany"
    ## the nodes must have a label with the key wc-node-select-flag
    ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
    nodeLabel: ""
    ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
    ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
    nodeSelector: {}
    coresSharingPersistentVolumeClaim: ""

searchAppRepeater:
  name: search-app-repeater
  image: commerce/search-app
  ## CAUTION: MAJOR UPDATE IN COMMERCE 9.1.18.0 SOLR-BASED SEARCH
  ## In version 9.1.18.0, Solr-based search updates the Solr version from 7.3.0 to 9.7.0
  ## This introduces significant changes, please check below "Upgrading Solr version 7.3.0 to 9.7.0" link for details
  ## https://help.hcl-software.com/commerce/9.1.0/search/concepts/c970_solrupgrade.html
  ## 
  ## Deployment Options for Solr-based Search in 9.1.18.0
  ## Option 1: Use 9.1.18.0 Solr-based search
  ##   - Please review "Upgrading Solr version 7.3.0 to 9.7.0" page carefully
  ##   - Ensure you set ${VAULT_URL}/${TENANT}/${ENVIRONMENT}/softwareStack to open in Vault.
  ##     The corresponding environment variable is SOFTWARE_STACK.
  ##   - Update tag to 9.1.18.0
  ## 
  ## Option 2: Use 9.1.17.0 Solr-based search + 9.1.18.0 Commerce others
  ##   - This approach lets you upgrade to HCL Commerce 9.1.18.0 without adopting the Solr 9.7.0 update
  ##   - If you are using Oracle
  ##     Open a case with HCL Customer Support to request patch HC-60507
  ##     Build customized 9.1.17.0 search-app docker image with patch HC-60507
  ##   - Update tag to your 9.1.17.0 search-app
  tag: 
  resources:
    requests:
      memory: 4096Mi
      cpu: 500m
    limits:
      memory: 4096Mi
      cpu: 2
  ## when using custom envParameters, use key: value format
  envParameters: {}
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## persistentVolumeClaim is deprecated and will be removed in future version
  ## Please specify the existing pvc in searchAppRepeater.persistence.existingClaim instead
  ## searchAppRepeater.persistence.existingClaim will take precedence if specified
  persistentVolumeClaim: ""
  persistence:
    enabled: true
    existingClaim: ""
    # keep storageClass empty to use the cluster default storageClass
    storageClass: ""
    storage: "5Gi"
    accessMode: "ReadWriteOnce"
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}
  coresSharingPersistentVolumeClaim: ""
  userScripts:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}
  ## demoPack is only used in SoFy sandbox, keep it disabled in your deployment
  demoPack:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}

searchAppSlave:
  name: search-app-slave
  replica: 1
  image: commerce/search-app
  ## CAUTION: MAJOR UPDATE IN COMMERCE 9.1.18.0 SOLR-BASED SEARCH
  ## In version 9.1.18.0, Solr-based search updates the Solr version from 7.3.0 to 9.7.0
  ## This introduces significant changes, please check below "Upgrading Solr version 7.3.0 to 9.7.0" link for details
  ## https://help.hcl-software.com/commerce/9.1.0/search/concepts/c970_solrupgrade.html
  ## 
  ## Deployment Options for Solr-based Search in 9.1.18.0
  ## Option 1: Use 9.1.18.0 Solr-based search
  ##   - Please review "Upgrading Solr version 7.3.0 to 9.7.0" page carefully
  ##   - Ensure you set ${VAULT_URL}/${TENANT}/${ENVIRONMENT}/softwareStack to open in Vault.
  ##     The corresponding environment variable is SOFTWARE_STACK
  ##   - Update tag to 9.1.18.0
  ## 
  ## Option 2: Use 9.1.17.0 Solr-based search + 9.1.18.0 Commerce others
  ##   - This approach lets you upgrade to HCL Commerce 9.1.18.0 without adopting the Solr 9.7.0 update
  ##   - If you are using Oracle
  ##     Open a case with HCL Customer Support to request patch HC-60507
  ##     Build customized 9.1.17.0 search-app docker image with patch HC-60507
  ##   - Update tag to your 9.1.17.0 search-app
  tag: 
  resources:
    requests:
      memory: 4096Mi
      cpu: 500m
    limits:
      memory: 4096Mi
      cpu: 2
  ## when using custom envParameters, use key: value format
  envParameters: {}
  persistence:
    ## when persistence is enabled. it will deploy search slave in StatefulSet. Otherwise Deployment will be created.
    enabled: false
    # keep storageClass empty to use the cluster default storageClass
    storageClass: ""
    storage: "5Gi"
    accessMode: "ReadWriteOnce"
  ## health check mode could be 'startup' or 'readiness'
  ##  'startup' mode will cross check search repeater as well
  ##  'readiness' mode will only check slave itself without checking repeater
  healthCheckMode: startup
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}
  coresSharingPersistentVolumeClaim: ""
  userScripts:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}
  ## demoPack is only used in SoFy sandbox, keep it disabled in your deployment
  demoPack:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}

  
orchestrationApp:
  enabled: false
  name: orchestration-app
  image: commerce/orchestration-app
  tag: 9.1.18.0
  replica:
    auth: 1
    live: 1
  resources:
    auth:
      requests:
        memory: 4096Mi
        cpu: 500m
      limits:
        memory: 4096Mi
        cpu: 2
    live:
      requests:
        memory: 4096Mi
        cpu: 500m
      limits:
        memory: 4096Mi
        cpu: 2
  ## when using custom envParameters, use key: value format
  envParameters:
    auth: {}
    live: {}
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector:
    auth: {}
    live: {}
  ## Persistent volume claim for Orchestration app cores dump
  coresSharingPersistentVolumeClaim:
    auth: ""
    live: ""
  userScripts:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}

tsWeb:
  name: ts-web
  replica: 1
  image: commerce/ts-web
  tag: 9.1.18.0
  resources:
    requests:
      memory: 2048Mi
      cpu: 500m
    limits:
      memory: 2048Mi
      cpu: 2
  ## when using custom envParameters, use key: value format
  envParameters: {}
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}

toolingWeb:
  name: tooling-web
  replica: 1
  image: commerce/tooling-web
  tag: 9.1.18.0
  resources:
    requests:
      memory: 2048Mi
      cpu: 500m
    limits:
      memory: 2048Mi
      cpu: 2
  ## when using custom envParameters, use key: value format
  envParameters: {}
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}
  ## extra csp source
  extraCspSource: ""

  
storeWeb:
  enabled: true
  name: store-web
  replica: 1
  image: commerce/store-web
  tag: 9.1.18.0
  resources:
    requests:
      memory: 2048Mi
      cpu: 500m
    limits:
      memory: 2048Mi
      cpu: 2
  ## when using custom envParameters, use key: value format
  envParameters:
    auth: {}
    live: {}
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}

nextjsApp:
  name: nextjs-app
  enabled: true
  replica: 1
  image: commerce/nextjs-store-app
  tag: 9.1.18.0
  resources:
    requests:
      memory: 2048Mi
      cpu: 2
    limits:
      memory: 2048Mi
      cpu: 2
  ## when using custom envParameters, use key: value format
  envParameters: 
    auth: {}
    live: {}
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}

crsApp:
  enabled: true
  name: crs-app
  image: commerce/crs-app
  tag: 9.1.18.0
  replica: 1
  resources:
    requests:
      memory: 4096Mi
      cpu: 500m
    limits:
      memory: 4096Mi
      cpu: 2
  ## when using custom envParameters, use key: value format
  envParameters:
    auth: {}
    live: {}
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}
  coresSharingPersistentVolumeClaim:
    auth: ""
    live: ""
  userScripts:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}
  ## demoPack is only used in SoFy sandbox, keep it disabled in your deployment
  demoPack:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}

xcApp:
  enabled: true
  name: xc-app
  image: commerce/xc-app
  tag: 9.1.18.0
  replica: 1
  resources:
    requests:
      memory: 4096Mi
      cpu: 500m
    limits:
      memory: 4096Mi
      cpu: 2
  ## when using custom envParameters, use key: value format
  envParameters:
    auth: {}
    live: {}
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}
  coresSharingPersistentVolumeClaim:
    auth: ""
    live: ""
  userScripts:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}
  ## demoPack is only used in SoFy sandbox, keep it disabled in your deployment
  demoPack:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}

nifiApp:
  name: nifi-app
  image: commerce/search-nifi-app
  tag: 9.1.18.0
  replica: 1
  resources:
    requests:
      memory: 10240Mi
      cpu: 500m
    limits:
      memory: 10240Mi
      cpu: 2
  ## when using custom envParameters, use key: value format
  envParameters: {}
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## persistentVolumeClaim is deprecated and will be removed in future version
  ## Please specify the existing pvc in nifiApp.persistence.existingClaim instead
  ## nifiApp.persistence.existingClaim will take precedence if specified
  persistentVolumeClaim: ""
  persistence:
    enabled: true
    existingClaim: ""
    # keep storageClass empty to use the cluster default storageClass
    storageClass: ""
    storage: "10Gi"
    accessMode: "ReadWriteOnce"
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}
  coresSharingPersistentVolumeClaim: ""
  userScripts:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}
  ## demoPack is only used in SoFy sandbox, keep it disabled in your deployment
  demoPack:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}

registryApp:
  name: registry-app
  image: commerce/search-registry-app
  tag: 9.1.18.0
  resources:
    requests:
      memory: 2048Mi
      cpu: 500m
    limits:
      memory: 2048Mi
      cpu: 2
  ## when using custom envParameters, use key: value format
  envParameters: {}
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}
  coresSharingPersistentVolumeClaim: ""
  persistence:
    enabled: true
    existingClaim: ""
    # keep storageClass empty to use the cluster default storageClass
    storageClass: ""
    storage: "100Mi"
    accessMode: "ReadWriteOnce"

ingestApp:
  name: ingest-app
  image: commerce/search-ingest-app
  tag: 9.1.18.0
  replica: 1
  resources:
    requests:
      memory: 4096Mi
      cpu: 500m
    limits:
      memory: 4096Mi
      cpu: 2
  ## when using custom envParameters, use key: value format
  envParameters: {}
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}
  coresSharingPersistentVolumeClaim: ""
  userScripts:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}
  ## demoPack is only used in SoFy sandbox, keep it disabled in your deployment
  demoPack:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}

queryApp:
  name: query-app
  image: commerce/search-query-app
  tag: 9.1.18.0
  replica:
    auth: 1
    live: 1
    data: 1
  resources:
    auth:
      requests:
        memory: 4096Mi
        cpu: 500m
      limits:
        memory: 4096Mi
        cpu: 2
    live:
      requests:
        memory: 4096Mi
        cpu: 500m
      limits:
        memory: 4096Mi
        cpu: 2
    data:
      requests:
        memory: 3072Mi
        cpu: 500m
      limits:
        memory: 3072Mi
        cpu: 2
  ## when using custom envParameters, use key: value format
  envParameters:
    auth: {}
    live: {}
    data: {}
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector:
    auth: {}
    live: {}
    data: {}
  coresSharingPersistentVolumeClaim:
    auth: ""
    live: ""
    data: ""
  userScripts:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}
  ## demoPack is only used in SoFy sandbox, keep it disabled in your deployment
  demoPack:
    enabled: false
    initContainer:
      image: ''
      tag: ''
      command: []
      args: []
      env: {}

cacheApp:
  name: cache-app
  enabled: false
  replica: 1
  image: commerce/cache-app
  tag: 9.1.18.0
  resources:
    requests:
      memory: 2048Mi
      cpu: 500m
    limits:
      memory: 2048Mi
      cpu: 2
  envParameters: {}
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}
  coresSharingPersistentVolumeClaim:
    auth: ""
    live: ""

graphqlApp:
  name: graphql-app
  enabled: true
  replica: 1
  image: commerce/graphql-app
  tag: 9.1.18.0
  resources:
    requests:
      memory: 2048Mi
      cpu: 500m
    limits:
      memory: 2048Mi
      cpu: 2
  envParameters: {}
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}

## Note: Must-Gather application is NOT production-ready
## Please keep enabled to be false in deployment
mustgatherApp:
  name: mustgather-app
  enabled: false
  replica: 1
  image: commerce/mustgather-app
  tag: 9.1.18.0
  resources:
    requests:
      memory: 4096Mi
      cpu: 500m
    limits:
      memory: 4096Mi
      cpu: 1
  envParameters: {}
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}
  coresSharingPersistentVolumeClaim: ""

tsUtils:
  enabled: false
  name: ts-utils
  image: commerce/ts-utils
  tag: 9.1.18.0
  resources:
    requests:
      memory: 4096Mi
      cpu: 500m
    limits:
      memory: 4096Mi
      cpu: 2
  ## when using custom envParameters, use key: value format
  envParameters:
    auth: {}
    live: {}
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}
  coresSharingPersistentVolumeClaim:
    auth: ""
    live: ""
  initContainer:
    enabled: false
    image: ""
    tag: ""
    command: []
    args: []

approvalApp:
  enabled: false
  name: approval-app
  image: commerce/approval-app
  tag: 9.1.18.0
  resources:
    requests:
      memory: 2048Mi
      cpu: 500m
    limits:
      memory: 2048Mi
      cpu: 2
  ## when using custom envParameters, use key: value format
  envParameters: {}
  ## the nodes must have a label with the key wc-node-select-flag
  ## then put the value of the label here to constrain which nodes your Pod can be scheduled on based on node labels
  nodeLabel: ""
  ## use nodeSelector to constrain a Pod so that it can only run on particular set of node(s)
  ## reference here for its format: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}
  ## boot Configurations for approval-app, you can choose to create a secret by yourself
  ## with the same format as app-secure.properties section and put the secret name in existingSecret
  ## or keep the existingSecret empty and let the helm chart create it automatically
  bootConfig:
    ## For security purpose, we would recommend creating a secret by yourself, and paste the name here
    ## Please do update the password in your postgresql database instead of using the sample one here
    existingSecret: ""
    app-secure.properties: |-
      spring.datasource.url=jdbc:postgresql://hcl-commerce-postgresql:5432/comdb
      spring.datasource.username=hclcom
      spring.datasource.password=passw0rd
      spring.datasource.hikari.schema=hclcom
      spring.flyway.url=jdbc:postgresql://hcl-commerce-postgresql:5432/comdb
      spring.flyway.user=hclcom
      spring.flyway.password=passw0rd
      spring.flyway.default-schema=hclcom

supportC:
  image: commerce/supportcontainer
  tag: 9.1.18.0

## Specify the Docker image for Helm test. centos:latest is the default Docker image that is used for Helm test.
test:
  image: docker.io/centos:latest
